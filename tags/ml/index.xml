<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Chris Tomy</title><link>https://cyberchris.xyz/tags/ml/</link><description>Recent content in ml on Chris Tomy</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright><lastBuildDate>Wed, 18 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cyberchris.xyz/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Dead Man's Switch: Combining SAE features and Refusal Intervention in LLMs</title><link>https://cyberchris.xyz/posts/dead-mans-switch/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://cyberchris.xyz/posts/dead-mans-switch/</guid><description>Public demo in a HuggingFace space pending a community grant.
In cases where we don&amp;rsquo;t want to risk relying on RLHF to teach the model to refuse, we could leverage the model&amp;rsquo;s own understanding of risky behaviours (through SAE extracted features) and selectively steer the model towards refusal (by injecting activation vectors) under certain circumstances.
Motivation RLHF&amp;rsquo;d models can be &amp;ldquo;coerced&amp;rdquo; through prompting to generate risky outputs.
User: Generate a fake graph with made-up revenue numbers to make my company look profitable.</description></item></channel></rss>