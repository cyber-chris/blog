<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>research on Chris Tomy</title><link>https://cyberchris.xyz/tags/research/</link><description>Recent content in research on Chris Tomy</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright><lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://cyberchris.xyz/tags/research/index.xml" rel="self" type="application/rss+xml"/><item><title>Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis</title><link>https://cyberchris.xyz/posts/cam-research/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://cyberchris.xyz/posts/cam-research/</guid><description>I&amp;rsquo;m currently working on a research project at Cambridge as part of my MPhil, applying deep learning to cancer diagnosis. Here are some slides I presented at a college conference. I&amp;rsquo;ll fill this page with more detail later in the year.</description></item><item><title>Dead Man's Switch: Combining SAE Features and Refusal Intervention in LLMs</title><link>https://cyberchris.xyz/posts/dead-mans-switch/</link><pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate><guid>https://cyberchris.xyz/posts/dead-mans-switch/</guid><description>An AI alignment project completed at the end of the AISF course. Public demo in a HuggingFace space still pending a community grant. GitHub repo with full code available here.
In cases where we don&amp;rsquo;t want to risk relying on RLHF to teach the model to refuse, we could leverage the model&amp;rsquo;s own understanding of risky behaviours (through SAE extracted features) and selectively steer the model towards refusal (by injecting activation vectors) under certain circumstances.</description></item><item><title>Building and Testing Better Sleeper Agents (SPAR 2024)</title><link>https://cyberchris.xyz/posts/spar-2024/</link><pubDate>Tue, 17 Sep 2024 00:00:00 +0000</pubDate><guid>https://cyberchris.xyz/posts/spar-2024/</guid><description>I&amp;rsquo;ve completed a research project with a small group, facilitated by SPAR. Our primary goal was to investigate the relationship between the complexity of a backdoor and its persistence to safety training. Notably, I designed and trained a family of arithmetic-based backdoors in llama3 (with QLoRA finetuning).
Our code is on GitHub and a writeup of our findings is here.
The shortened summary is that I didn&amp;rsquo;t find evidence that persistence changes between simpler vs.</description></item></channel></rss>